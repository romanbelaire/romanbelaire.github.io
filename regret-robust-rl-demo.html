<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Regret-Based Robust RL</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="#" class="title">Regret-Based Defense in Adversarial Reinforcement Learning</a>
				<nav>
					<ul>
						<li><a href="#">Github</a></li>
						<li><a href="https://arxiv.org/abs/2302.06912">Paper</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
              <b>Abstract:</b> Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable to small adversarial noise in observations. Such adversarial noise can have disastrous consequences in safety-critical environments. For instance, a self-driving car receiving adversarially perturbed sensory observations about nearby signs (e.g., a stop sign physically altered to be perceived as a speed limit sign) or objects (e.g., cars altered to be recognized as trees) can be fatal. Existing approaches for making RL algorithms robust to an observation-perturbing adversary have focused on reactive approaches that iteratively improve against adversarial examples generated at each iteration. While such approaches have been shown to provide improvements over regular RL methods, they are reactive and can fare significantly worse if certain categories of adversarial examples are not generated during training. To that end, we pursue a more proactive approach that relies on directly optimizing a well-studied robustness measure, regret instead of expected value. We provide a principled approach that minimizes maximum regret over a "neighborhood" of observations to the received "observation". Our regret criterion can be used to modify existing value- and policy-based Deep RL methods. We demonstrate that our approaches provide a significant improvement in performance across a wide variety of benchmarks against leading approaches for robust Deep RL.
							</div>
					</section>

			</div>
      <section id="intro" class="wrapper style1 fullscreen fade-up">
        <div class="inner" style="margin:0 auto">
          <h2>Finding Regret-Minimizing Policies for Adversarially Robust RL:</h2>
          <p style="color:white">In our investigation, we present innovative methodologies for computing Reinforcement Learning (RL) policies designed to exhibit resilience in the face of perturbations to their behavioral dynamics. Diverging from traditional paradigms that prioritize the maximization of rewards, our approach centers on the minimization of regret within the enacted policies.
<br> &nbsp&nbsp&nbsp&nbsp Conventional state-of-the-art techniques typically gravitate towards either the maximization of a robust, guaranteed minimum reward or the fortification of a classifier/neural network to mitigate errors. However, both methodologies employ the optimization of reward signals. Within the framework of game theory, our proposed methodology can be aptly classified as "mini-max regret," diverging fundamentally from the prevalent "maxi-min reward" strategies employed by existing approaches. <br>
&nbsp&nbsp&nbsp&nbsp
Policies grounded in regret-based optimization yield outcomes characterized by enhanced security and consistency, a phenomenon demonstrated in a visual representation provided below.</p>
          <center>
          <table>
            <tr>
              <th>Vanilla PPO</th>
              <th>Trial Description</th>
              <th>RAD-PPO</th>
            </tr>
            <tr>
              <th></th>
              <th>Mujoco: HalfCheetah</th>
            </tr>
            <tr>
              <td><img src="regret-images/vanilla_ppo_cheetah.gif" alt="" width=300px style="vertical-align:top"></td>
              <td>Unperturbed Test: The tangible difference between the conventional (vanilla) and robust policies is readily apparent in this task. The vanilla policy sustains higher velocity through a distinctive locomotion pattern, leveraging its rear leg to "scoot" forwards while elevating the front leg. Conversely, the robust policy adopts a more measured and stable gallop-like strategy with both legs contacting the floor. In this trial, the vanilla policy achieves a 30% further distance than the robust agent.
 </td>
              <td><img src="regret-images/soft_ccer_cheetah.gif" alt="" width=300px style="vertical-align:top"></td>
            </tr>
            <tr>
              <td><img src="regret-images/vanilla_ppo_cheetah_atk.gif" alt="" width=300px style="vertical-align:top"></td>
              <td>Perturbed Test: Here, we see the consequences of each strategy. The vanilla policy is unstable and repeatedly "faceplants", even flipping over in the third episode. The robust policy manages to maintain its stable gait, with only slight stuttering. The robust policy scores double that of the vanilla policy on average, even when excluding immediate failures by the vanilla policy. </td>
              <td><img src="regret-images/soft_ccer_cheetah_critic.gif" alt="" width=300px style="vertical-align:top"></td>
            </tr>
            <tr>
              <th></th>
              <th>Mujoco: Walker2D</th>
            </tr>
            <tr>
              <td><img src="regret-images/vanilla_ppo_walker.gif" alt="" width=300px style="vertical-align:top"></td>
              <td>Unperturbed Test: Again, we can observe a distinctly different strategy between the two approaches. The value-optimizing agent uses both feet to contact the ground and leap forwards, while the regret-minimizing agent uses one leg as a counter balance. The robust strategy has a slower start, but appears to eventually match the top speed, scoring around the same on average. </td>
              <td><img src="regret-images/soft_ccer_walker.gif" alt="" width=300px style="vertical-align:top"></td>
            </tr>
            <tr>
              <td><img src="regret-images/vanilla_ppo_walker_atk.gif" alt="" width=300px style="vertical-align:top"></td>
              <td>Perturbed Test: Once again we see that the instability of the vanilla agent's strategy leads to over-correction when perturbations occur. While difficult to notice at a glance, the vanilla policy "kicks" more widely to recover as it becomes more unstable. </td>
              <td><img src="regret-images/soft_ccer_walker_critic.gif" alt="" width=300px style="vertical-align:top"></td>
            </tr>
            <tr>
              <th></th>
              <th>Mujoco: Hopper</th>
            </tr>
            <tr>
              <td><img src="regret-images/vanilla_ppo_hopper.gif" alt="" width=300px style="vertical-align:top"></td>
              <td>Unperturbed Test: Differences between strategies in this task are the hardest to distinguish visually, though when comparing the forward tilt of each agent's body one will notice the vanilla agent leaning more to generate more horizontal momentum. Overall, the vanilla policy scores slightly higher.</td>
              <td><img src="regret-images/soft_ccer_hopper.gif" alt="" width=300px style="vertical-align:top"></td>
            </tr>
            <tr>
              <td><img src="regret-images/vanilla_ppo_hopper_atk.gif" alt="" width=300px style="vertical-align:top"></td>
              <td>Perturbed Test: Even though the two strategies are largely similar, adversarial perturbations exacerbate the slight difference in stability: the difference in forward tilt is even more noticeable, culminating in an early failure for the nonrobust agent.</td>
              <td><img src="regret-images/soft_ccer_hopper_critic.gif" alt="" width=300px style="vertical-align:top"></td>
            </tr>
          </table>
          </center>
        </div>
      </section>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy All rights reserved. Please cite any useage of our work or images :) </li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
